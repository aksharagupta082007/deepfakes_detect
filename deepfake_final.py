# -*- coding: utf-8 -*-
"""Copy of deepfake_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kd9B6A_ipymTyG-hy8HHpn-yjncS-ga9
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("reubensuju/celeb-df-v2")

print("Path to dataset files:", path)

import os
import kagglehub
import shutil

# Step 1: Download entire dataset
dataset_path = kagglehub.dataset_download("xdxd003/ff-c23")

import kagglehub

# Download latest version
path = kagglehub.dataset_download("sanikatiwarekar/deep-fake-detection-dfd-entire-original-dataset")

print("Path to dataset files:", path)

import os
import shutil
import random
from math import floor
from tqdm import tqdm

# Paths to datasets
celeb_df_path = "/kaggle/input/celeb-df-v2" # Corrected path
ff_c23_path = "/content/ff-c23"  # ff-c23 original folder not found in previous execution
dfd_path = "/kaggle/input/deep-fake-detection-dfd-entire-original-dataset" # Corrected path

# Create merged dataset structure
base_path = "/content/merged_dataset"
splits = ["train", "val", "test"]
categories = ["original", "deepfake"]

for split in splits:
    for cat in categories:
        os.makedirs(os.path.join(base_path, split, cat), exist_ok=True)


#  Collect video lists


# Celeb-DF v2
celeb_synth = [os.path.join(celeb_df_path, "Celeb-synthesis", f)
               for f in os.listdir(os.path.join(celeb_df_path, "Celeb-synthesis"))]
celeb_real = [os.path.join(celeb_df_path, f, v)
              for f in ["Celeb-real", "YouTube-real"]
              for v in os.listdir(os.path.join(celeb_df_path, f))]

# FF-C23 originals - Skipping due to previous error
# ff_original = [os.path.join(ff_c23_path, "original", f)
#                for f in os.listdir(os.path.join(ff_c23_path, "original"))]

# DFD originals
dfd_originals = [os.path.join(dfd_path, "DFD_original sequences", f)
                 for f in os.listdir(os.path.join(dfd_path, "DFD_original sequences"))]

# Combine all originals
all_originals = celeb_real + dfd_originals # Removed ff_original
all_deepfakes = celeb_synth


# Balance datasets

min_count = min(len(all_originals), len(all_deepfakes))
all_originals = random.sample(all_originals, min_count)
all_deepfakes = random.sample(all_deepfakes, min_count)


# Split into train/val/test

train_count = floor(0.7 * min_count)
val_count = floor(0.15 * min_count)
test_count = min_count - train_count - val_count

splits_idx = {
    "train": (0, train_count),
    "val": (train_count, train_count + val_count),
    "test": (train_count + val_count, min_count)
}


#  Copy videos to respective folders with progress bar

for split, (start, end) in splits_idx.items():
    print(f"\nCopying videos for {split} set:")

    # Originals
    print("Original videos:")
    for src in tqdm(all_originals[start:end], desc="Copying originals", ncols=100):
        dst = os.path.join(base_path, split, "original", os.path.basename(src))
        shutil.copy(src, dst)

    # Deepfakes
    print("Deepfake videos:")
    for src in tqdm(all_deepfakes[start:end], desc="Copying deepfakes", ncols=100):
        dst = os.path.join(base_path, split, "deepfake", os.path.basename(src))
        shutil.copy(src, dst)

print("\nDataset merged and split successfully!")
for split in splits:
    for cat in categories:
        folder = os.path.join(base_path, split, cat)
        print(f"{split}/{cat}: {len(os.listdir(folder))} videos")

!pip install opencv-python-headless torch torchvision transformers albumentations tqdm



import os, cv2, random, shutil
from math import floor
from tqdm import tqdm
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import numpy as np

from transformers import ViTModel
import timm
from torch.cuda.amp import autocast, GradScaler

# Define image size and number of frames
IMG_SIZE = 224
FRAMES_PER_VIDEO = 16

PROCESSED_DIR = "/content/processed_dataset"
os.makedirs(PROCESSED_DIR, exist_ok=True)

labels_map = {"original":0, "deepfake":1}

transform = transforms.Compose([
    transforms.Resize((IMG_SIZE,IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485,0.456,0.406],
                         std=[0.229,0.224,0.225])
])

def load_video(path, num_frames=FRAMES_PER_VIDEO):
    cap = cv2.VideoCapture(path)
    frames,total = [], int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    step = max(total // num_frames, 1)
    i=0
    while cap.isOpened() and len(frames)<num_frames:
        ret,frame = cap.read()
        if not ret: break
        if i%step==0:
            img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
            frames.append(transform(img))
        i+=1
    cap.release()
    if len(frames)==0:
        frames = [torch.zeros(3,IMG_SIZE,IMG_SIZE)]*num_frames
    while len(frames)<num_frames:
        frames.append(frames[-1])
    return torch.stack(frames)

from PIL import UnidentifiedImageError

def preprocess_dataset(src_root,dst_root,labels_map,start_idx=0):
    for label_name,label_id in labels_map.items():
        folder = os.path.join(src_root,label_name)
        save_folder = os.path.join(dst_root,label_name)
        os.makedirs(save_folder, exist_ok=True)

        files = sorted(os.listdir(folder))
        # skip files already processed
        existing = set(f.replace(".pt","") for f in os.listdir(save_folder) if f.endswith(".pt"))
        files = [f for f in files if f not in existing]
        files = files[start_idx:]

        print(f"{label_name}: {len(existing)} already done, resuming from index {start_idx}")

        for fname in tqdm(files, desc=f"Preprocessing {label_name}"):
            path = os.path.join(folder,fname)

            try:
                if fname.lower().endswith((".mp4",".avi",".mov",".mkv")):
                    tensor = load_video(path)
                elif fname.lower().endswith((".jpg",".jpeg",".png")):
                    img = Image.open(path).convert("RGB")
                    tensor = transform(img).unsqueeze(0).repeat(FRAMES_PER_VIDEO,1,1,1)
                else:
                    continue
            except (UnidentifiedImageError,OSError) as e:
                print(f"Skipping corrupted file: {path}")
                continue

            torch.save({"video":tensor,"label":label_id}, os.path.join(save_folder,fname+".pt"))

# Run once for each split
for split in splits:
    preprocess_dataset(os.path.join(base_path,split), os.path.join(PROCESSED_DIR,split), labels_map)


# 1. Import
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from transformers import ViTModel
import timm
from tqdm import tqdm

# Device setup (use GPU if available)
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print("Using device:", DEVICE)

# Temporal Transformer 
class TemporalTransformer(nn.Module):
    def __init__(self, num_frames=16, embed_dim=512, num_heads=8, depth=4, dropout=0.1):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_frames = num_frames

        # CLS token
        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))

        # Positional embeddings
        self.pos_embed = nn.Parameter(torch.randn(1, num_frames + 1, embed_dim))

        # Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim,
            nhead=num_heads,
            dim_feedforward=embed_dim * 4,
            dropout=dropout,
            batch_first=True,
            norm_first=True,   # Pre-Norm
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=depth)

        self.dropout = nn.Dropout(dropout)
        self.norm = nn.LayerNorm(embed_dim)

    def forward(self, x):
        # x: [B, T, C]
        B, T, C = x.shape

        # Add CLS token
        cls_tokens = self.cls_token.expand(B, -1, -1)   # [B,1,C]
        x = torch.cat([cls_tokens, x], dim=1)           # [B,T+1,C]

        # Add positional embeddings
        pos = self.pos_embed[:, :T+1, :]
        x = x + pos

        # Transformer
        x = self.dropout(x)
        x = self.encoder(x)                             # [B,T+1,C]

        # CLS output
        out = self.norm(x[:, 0, :])                     # [B,C]
        return out

# 3. Model Definition (Hybrid DenseNet + ViT + Temporal)

class HybridDenseNetViT(nn.Module):
    def __init__(self):
        super().__init__()
      
        self.cnn = timm.create_model("densenet121", pretrained=True,
                                     num_classes=0, global_pool="avg")
        cnn_out = self.cnn.num_features

    
        self.vit = ViTModel.from_pretrained("google/vit-base-patch16-224")
        vit_out = self.vit.config.hidden_size


        self.proj_cnn = nn.Linear(cnn_out, 256)
        self.proj_vit = nn.Linear(vit_out, 256)

    
        self.temporal = TemporalTransformer(embed_dim=512, num_heads=8, depth=4)


        self.fc = nn.Linear(512, 2)

    def forward(self, x):
       
        B, T, C, H, W = x.shape
        x = x.view(B*T, C, H, W)

 
        cnn_feat = self.proj_cnn(self.cnn(x))                          # [B*T, 256]

    
        vit_feat = self.proj_vit(self.vit(x).last_hidden_state[:,0,:]) # [B*T, 256]

        feat = torch.cat([cnn_feat, vit_feat], dim=1)                  # [B*T, 512]
        feat = feat.view(B, T, -1)                                     # [B, T, 512]

        feat = self.temporal(feat)                                     # [B, 512]

        return self.fc(feat)

# Instantiate model
model = HybridDenseNetViT().to(DEVICE)
print("Model with Temporal Transformer loaded ")



# 4. Training Utilities
from sklearn.metrics import roc_auc_score

def evaluate(model, loader):
    model.eval()
    correct, total = 0, 0
    all_preds, all_labels = [], []

    with torch.no_grad():
        for x, y in loader:
            x, y = x.to(DEVICE), y.to(DEVICE)
            logits = model(x)
            preds = logits.argmax(1)

            correct += (preds == y).sum().item()
            total += y.size(0)

            all_preds.extend(torch.softmax(logits,1)[:,1].cpu().numpy())
            all_labels.extend(y.cpu().numpy())

    acc = correct / total
    try:
        auc = roc_auc_score(all_labels, all_preds)
    except:
        auc = 0.0
    return acc, auc

from torch.amp import GradScaler, autocast
scaler = GradScaler("cuda")

LR_CNN = 1e-4
LR_VIT = 5e-5
EPOCHS_STAGE1 = 5
EPOCHS_STAGE2 = 5
EPOCHS_STAGE3 = 3
best_acc = 0

# Stage 1: Train CNN + Projections + Temporal + FC (ViT frozen)
print("\n Stage 1: Training CNN + Projection + Temporal + FC (ViT frozen)")
for param in model.vit.parameters():
    param.requires_grad = False

optimizer = torch.optim.AdamW([
    {"params": model.cnn.parameters(), "lr": LR_CNN},
    {"params": model.proj_cnn.parameters(), "lr": LR_CNN},
    {"params": model.proj_vit.parameters(), "lr": LR_CNN},
    {"params": model.temporal.parameters(), "lr": LR_CNN},  # NEW
    {"params": model.fc.parameters(), "lr": LR_CNN},
], weight_decay=1e-5)

for epoch in range(EPOCHS_STAGE1):
    model.train()
    loop = tqdm(train_loader, desc=f"Stage1 Epoch {epoch+1}")
    for x,y in loop:
        x,y = x.to(DEVICE), y.to(DEVICE)
        optimizer.zero_grad()
        with autocast("cuda"):
            logits = model(x)
            loss = F.cross_entropy(logits,y)
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
    acc, auc = evaluate(model, val_loader)
    print(f"Val Acc: {acc:.4f} | AUC: {auc:.4f}")
    if acc > best_acc:
        torch.save(model.state_dict(),"best_model.pth")
        best_acc = acc

# Stage 2: Fine-tune last 2 ViT layers

print("\n Stage 2: Fine-tuning last 2 ViT blocks")
for param in model.vit.encoder.layer[-2:].parameters():
    param.requires_grad = True

optimizer = torch.optim.AdamW([
    {"params": model.cnn.parameters(), "lr": LR_CNN},
    {"params": model.proj_cnn.parameters(), "lr": LR_CNN},
    {"params": model.proj_vit.parameters(), "lr": LR_CNN},
    {"params": model.temporal.parameters(), "lr": LR_CNN},
    {"params": model.fc.parameters(), "lr": LR_CNN},
    {"params": model.vit.encoder.layer[-2:].parameters(), "lr": LR_VIT},
], weight_decay=1e-5)

for epoch in range(EPOCHS_STAGE2):
    model.train()
    loop = tqdm(train_loader, desc=f"Stage2 Epoch {epoch+1}")
    for x,y in loop:
        x,y = x.to(DEVICE), y.to(DEVICE)
        optimizer.zero_grad()
        with autocast("cuda"):
            logits = model(x)
            loss = F.cross_entropy(logits,y)
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
    acc, auc = evaluate(model, val_loader)
    print(f"Val Acc: {acc:.4f} | AUC: {auc:.4f}")
    if acc > best_acc:
        torch.save(model.state_dict(),"best_model.pth")
        best_acc = acc

from google.colab import drive
drive.mount('/content/drive')

# Create a folder in Drive to store the model
save_path = "/content/drive/MyDrive/deepfake_models"
os.makedirs(save_path, exist_ok=True)

# Copy the best model checkpoint
!cp /content/best_model.pth $save_path/

print(" Model saved to:", save_path)

# Stage 3: Full fine-tune
print("\n🔹 Stage 3: Full fine-tuning ViT (small LR)")
for param in model.vit.parameters():
    param.requires_grad = True

optimizer = torch.optim.AdamW([
    {"params": model.parameters(), "lr": LR_VIT},  # full model
], weight_decay=1e-5)

for epoch in range(EPOCHS_STAGE3):
    model.train()
    loop = tqdm(train_loader, desc=f"Stage3 Epoch {epoch+1}")
    for x,y in loop:
        x,y = x.to(DEVICE), y.to(DEVICE)
        optimizer.zero_grad()
        with autocast("cuda"):
            logits = model(x)
            loss = F.cross_entropy(logits,y)
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
    acc, auc = evaluate(model, val_loader)
    print(f"Val Acc: {acc:.4f} | AUC: {auc:.4f}")
    if acc > best_acc:
        torch.save(model.state_dict(),"best_model.pth")
        best_acc = acc

print("\nTraining complete! Best Val Acc:", best_acc)

# Reload model
model = HybridDenseNetViT().to(DEVICE)
model.load_state_dict(torch.load("/content/best_model.pth"))
model.eval()
print(" Model loaded successfully!")

import os
import torch
import torch.nn.functional as F
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
import random
from torchvision import transforms
from PIL import Image
from tqdm import tqdm


# 1. Config
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
TEST_DIR = "/content/processed_dataset/test"  
# 2. Helper: Load preprocessed tensor

def load_preprocessed_tensor(filepath):
    """Loads a preprocessed tensor and its label from a .pt file."""
    data = torch.load(filepath, map_location="cpu")
    return data["video"], data["label"] # "video" is actually the tensor of frames


# 3. Iterate through test set

y_true, y_pred = [], []

classes = ["original", "deepfake"]

print("Evaluating on test set...")
for label, cls in enumerate(classes):
    cls_folder = os.path.join(TEST_DIR, cls)
    # Iterate through the .pt files directly
    for filename in tqdm(os.listdir(cls_folder), desc=f"Processing {cls}"):
        if filename.endswith(".pt"):
            filepath = os.path.join(cls_folder, filename)

            # Load preprocessed tensor and label
            frames_tensor, true_label = load_preprocessed_tensor(filepath)

            # Add batch dimension and move to device
            frames_tensor = frames_tensor.unsqueeze(0).to(DEVICE)  # (1, T, C, H, W)

            with torch.no_grad():
                logits = model(frames_tensor)
                prob = F.softmax(logits, dim=1).cpu().numpy()[0]
                pred = np.argmax(prob)

            y_true.append(true_label)
            y_pred.append(pred)


# 4. Report

print("\n Classification Report:")
print(classification_report(y_true, y_pred, target_names=classes, digits=4))

print("\n Confusion Matrix:")
print(confusion_matrix(y_true, y_pred))

import os

dfd_path = "/root/.cache/kagglehub/datasets/sanikatiwarekar/deep-fake-detection-dfd-entire-original-dataset/versions/1" # Path to the downloaded dataset
print("Files in DFD dataset:")
for root, dirs, files in os.walk(dfd_path):
    level = root.replace(dfd_path, '').count(os.sep)
    indent = ' ' * 4 * (level)
    print('{}{}/'.format(indent, os.path.basename(root)))
    subindent = ' ' * 4 * (level + 1)
    for f in files:
        print('{}{}'.format(subindent, f))

import os

def find_dfd_dataset(search_path='/'):
    """Searches for the DFD dataset directory containing 'DFD_original sequences'."""
    for root, dirs, files in os.walk(search_path):
        if "DFD_original sequences" in dirs:
            dfd_path = os.path.join(root, "DFD_original sequences")
            print(f"Found DFD original sequences directory at: {dfd_path}")
            return dfd_path
    print("DFD original sequences directory not found.")
    return None


find_dfd_dataset('/')